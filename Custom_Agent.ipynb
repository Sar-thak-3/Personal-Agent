{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "rG_O8dCVKsS9",
        "kqJ-jPV4AJtH",
        "XcqwPaNJAP4H"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing Data\n",
        "\n",
        "Process the data from URLs using BeatifulSoup and stores them as maps of content, relevant links, tables, Lecture URL and pargraph number."
      ],
      "metadata": {
        "id": "rG_O8dCVKsS9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Libraries\n",
        "\n",
        "import markdown\n",
        "import requests\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import requests\n",
        "from bs4 import BeautifulSoup"
      ],
      "metadata": {
        "id": "7EEt_ncPWJUf"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install groq langchain==0.1.16 langchain-core langchain-groq\n",
        "# Langchains used as LLMs in project"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dYvZLyd_5bt0",
        "outputId": "6c934e2f-4faa-42fe-cb4d-0bed82200066"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting groq\n",
            "  Downloading groq-0.9.0-py3-none-any.whl (103 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.5/103.5 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain==0.1.16\n",
            "  Downloading langchain-0.1.16-py3-none-any.whl (817 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m817.7/817.7 kB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-core\n",
            "  Downloading langchain_core-0.2.7-py3-none-any.whl (315 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m315.6/315.6 kB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-groq\n",
            "  Downloading langchain_groq-0.1.5-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.16) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.16) (2.0.30)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.16) (3.9.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.16) (4.0.3)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain==0.1.16)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain==0.1.16)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting langchain-community<0.1,>=0.0.32 (from langchain==0.1.16)\n",
            "  Downloading langchain_community-0.0.38-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-core\n",
            "  Downloading langchain_core-0.1.52-py3-none-any.whl (302 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.9/302.9 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-text-splitters<0.1,>=0.0.1 (from langchain==0.1.16)\n",
            "  Downloading langchain_text_splitters-0.0.2-py3-none-any.whl (23 kB)\n",
            "Collecting langsmith<0.2.0,>=0.1.17 (from langchain==0.1.16)\n",
            "  Downloading langsmith-0.1.77-py3-none-any.whl (125 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.2/125.2 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.16) (1.25.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.16) (2.7.3)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.16) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.16) (8.3.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from groq) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from groq) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from groq)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from groq) (4.12.2)\n",
            "Collecting packaging<24.0,>=23.2 (from langchain-core)\n",
            "  Downloading packaging-23.2-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.16) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.16) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.16) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.16) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.16) (1.9.4)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->groq) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->groq) (1.2.1)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain==0.1.16)\n",
            "  Downloading marshmallow-3.21.3-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.2/49.2 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain==0.1.16)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->groq) (2024.6.2)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->groq)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->groq)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain==0.1.16)\n",
            "  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
            "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.17->langchain==0.1.16)\n",
            "  Downloading orjson-3.10.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m145.0/145.0 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain==0.1.16) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain==0.1.16) (2.18.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.1.16) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.1.16) (2.0.7)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain==0.1.16) (3.0.3)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain==0.1.16)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: packaging, orjson, mypy-extensions, jsonpointer, h11, typing-inspect, marshmallow, jsonpatch, httpcore, langsmith, httpx, dataclasses-json, langchain-core, groq, langchain-text-splitters, langchain-groq, langchain-community, langchain\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 24.1\n",
            "    Uninstalling packaging-24.1:\n",
            "      Successfully uninstalled packaging-24.1\n",
            "Successfully installed dataclasses-json-0.6.7 groq-0.9.0 h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 jsonpatch-1.33 jsonpointer-3.0.0 langchain-0.1.16 langchain-community-0.0.38 langchain-core-0.1.52 langchain-groq-0.1.5 langchain-text-splitters-0.0.2 langsmith-0.1.77 marshmallow-3.21.3 mypy-extensions-1.0.0 orjson-3.10.5 packaging-23.2 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sections = []\n",
        "# Extracting the sections from urls of lectures and readme file"
      ],
      "metadata": {
        "id": "XT7GOjUFfOKn"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to extract text from HTML content\n",
        "def extract_text(html_content, url):\n",
        "    soup = BeautifulSoup(html_content, 'html.parser')\n",
        "    headers = ['h1', 'h2', 'h3', 'h4', 'h5', 'h6']\n",
        "\n",
        "    index = 1\n",
        "\n",
        "    for header_tag in soup.find_all(headers):\n",
        "        section_title = header_tag.text.strip()\n",
        "        content = ''\n",
        "        links = []\n",
        "        tables = []\n",
        "\n",
        "        for tag in header_tag.find_next_siblings():\n",
        "            if tag.name in headers:\n",
        "                break\n",
        "            for a_tag in tag.find_all('a'):\n",
        "                links.append(a_tag.get('href'))\n",
        "            if tag.name == 'table':\n",
        "                table_data = []\n",
        "                table_headers = [header.text.strip() for header in tag.find_all('th')]\n",
        "                table_data.append(table_headers)\n",
        "                for row in tag.find_all('tr'):\n",
        "                    row_data = [cell.text.strip() for cell in row.find_all('td')]\n",
        "                    if row_data:\n",
        "                        table_data.append(row_data)\n",
        "                tables.append(table_data)\n",
        "            else:\n",
        "                content += tag.get_text(strip=True)\n",
        "\n",
        "        # Storing the data as maps containing content of paragraph of lectures, url, title, relevant links in paragraph and paragraph number in url\n",
        "        sections.append({\n",
        "            'lecture_url': url,\n",
        "            'title': section_title,\n",
        "            'content': content,\n",
        "            'links': links,\n",
        "            'tables': tables,\n",
        "            'paragraph_number': index\n",
        "        })\n",
        "        index += 1\n",
        "\n",
        "    return\n",
        "\n",
        "# URL of the webpage\n",
        "# url = 'https://stanford-cs324.github.io/winter2022/lectures/introduction/'\n",
        "\n",
        "# # Send a GET request to the URL\n",
        "# response = requests.get(url)\n",
        "\n",
        "# # Check if the request was successful (status code 200)\n",
        "# if response.status_code == 200:\n",
        "#     # Get the HTML content of the webpage\n",
        "#     html_content = response.text\n",
        "\n",
        "#     # Extract text from HTML content\n",
        "#     sections = extract_text(html_content)\n",
        "\n",
        "#     # Print sections\n",
        "#     for section in sections:\n",
        "#         print(\"Title:\", section['title'])\n",
        "#         print(\"Content:\", section['content'])\n",
        "#         print(\"Links:\", section['links'])\n",
        "#         print(\"Tables:\", section['tables'])\n",
        "#         print()\n",
        "# else:\n",
        "#     print(\"Failed to retrieve HTML content. Status code:\", response.status_code)"
      ],
      "metadata": {
        "id": "F9jUiaZxWzwG"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extracting the html content from the url\n",
        "def extract_html_content(url):\n",
        "  # Send a GET request to the URL\n",
        "  response = requests.get(url)\n",
        "\n",
        "  # Check if the request was successful (status code 200)\n",
        "  if response.status_code == 200:\n",
        "      # Get the HTML content of the webpage\n",
        "      html_content = response.text\n",
        "      extract_text(html_content,url)\n",
        "      return\n",
        "  print(\"Failed to retrieve HTML content. Status code:\", response.status_code)\n",
        "  return"
      ],
      "metadata": {
        "id": "JeqcgTPbXEWX"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lectures of standford list\n",
        "lectures_list = [\n",
        "    'https://stanford-cs324.github.io/winter2022/lectures/introduction/',\n",
        "    'https://stanford-cs324.github.io/winter2022/lectures/capabilities/',\n",
        "    'https://stanford-cs324.github.io/winter2022/lectures/harms-1/',\n",
        "    'https://stanford-cs324.github.io/winter2022/lectures/harms-2/',\n",
        "    'https://stanford-cs324.github.io/winter2022/lectures/data/',\n",
        "    'https://stanford-cs324.github.io/winter2022/lectures/modeling/',\n",
        "    'https://stanford-cs324.github.io/winter2022/lectures/training/',\n",
        "    'https://stanford-cs324.github.io/winter2022/lectures/selective-architectures/',\n",
        "    'https://stanford-cs324.github.io/winter2022/lectures/adaptation/',\n",
        "    'https://stanford-cs324.github.io/winter2022/lectures/environment/',\n",
        "    ]\n",
        "# Readme files list\n",
        "md_file_list = ['https://raw.githubusercontent.com/Hannibal046/Awesome-LLM/main/README.md']"
      ],
      "metadata": {
        "id": "TqGzTNTKW5FI"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_readme_to_html(url):\n",
        "  # URL of the webpage\n",
        "\n",
        "  # Send a GET request to the URL\n",
        "  response = requests.get(url)\n",
        "\n",
        "  # Check if the request was successful (status code 200)\n",
        "  if response.status_code == 200:\n",
        "      # Get the text content of the webpage\n",
        "      markdown_text = response.text\n",
        "  else:\n",
        "      print(\"Failed to retrieve Markdown content. Status code:\", response.status_code)\n",
        "\n",
        "  html_content = markdown.markdown(markdown_text)\n",
        "  return html_content"
      ],
      "metadata": {
        "collapsed": true,
        "id": "FRr2LZrqIxaE"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Storing paragraphs of each url\n",
        "\n",
        "for url in lectures_list:\n",
        "  extract_html_content(url)\n",
        "\n",
        "for url in md_file_list:\n",
        "  extract_text(convert_readme_to_html(url),url)"
      ],
      "metadata": {
        "id": "Wq3EORVxYv77"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(sections)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q5gkRuaze96s",
        "outputId": "a80b33e5-71a9-4aae-e447-cb56904da9d3"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "133"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Method - 1\n",
        "This method utilize cosine similarity to check the number of paragraphs from data which best match the user input query, hence the current input already contain the data from which LLM need to answer the input query"
      ],
      "metadata": {
        "id": "kqJ-jPV4AJtH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "contents = [item['content'] for item in sections]"
      ],
      "metadata": {
        "id": "vRQDIbNb1HAm"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def check_similarity(num_top_paragraphs, query):\n",
        "\n",
        "  # Initialize TF-IDF vectorizer\n",
        "  list_of_maps = []\n",
        "\n",
        "  tfidf_vectorizer = TfidfVectorizer()\n",
        "\n",
        "  # Fit the vectorizer to the paragraphs and transform them into TF-IDF vectors\n",
        "  tfidf_matrix = tfidf_vectorizer.fit_transform(contents)\n",
        "\n",
        "  # Transform the user query into a TF-IDF vector\n",
        "  query_vector = tfidf_vectorizer.transform([query])\n",
        "\n",
        "  # Calculate cosine similarity between the query vector and paragraph vectors\n",
        "  cosine_similarities = cosine_similarity(query_vector, tfidf_matrix)\n",
        "\n",
        "  # Rank paragraphs based on cosine similarity scores\n",
        "  ranked_paragraphs = sorted(enumerate(cosine_similarities[0]), key=lambda x: -x[1])\n",
        "\n",
        "  for (idx, score) in ranked_paragraphs[:num_top_paragraphs]:\n",
        "        list_of_maps.append(sections[idx])\n",
        "\n",
        "  return list_of_maps"
      ],
      "metadata": {
        "id": "4NILUKdHWMib"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n = 2 # Declate the number here\n",
        "\n",
        "def enhanced_question(user_question):\n",
        "  similar_sections = check_similarity(n,user_question)\n",
        "  text = \"\"\n",
        "  text += user_question\n",
        "  unique_urls = []\n",
        "  text += \"/n Kindly answer the question based on the details provided below along with the links and tables, it contains certain contents, relevant links, and tables one by one. \\n\"\n",
        "  for item in similar_sections:\n",
        "    text += f\"Content: {item['content']} \\n\"\n",
        "    text += f\"Links: {item['links']} \\n\"\n",
        "    text += f\"Tables: {item['tables']} \\n\\n\"\n",
        "\n",
        "    if(item['lecture_url'] not in unique_urls):\n",
        "      unique_urls.append(item['lecture_url'])\n",
        "  print(unique_urls)\n",
        "  return text"
      ],
      "metadata": {
        "id": "nFZF-gDx0UoK"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "from langchain.chains import LLMChain\n",
        "from langchain_core.prompts import (\n",
        "    ChatPromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        "    MessagesPlaceholder,\n",
        ")\n",
        "from langchain_core.messages import SystemMessage\n",
        "from langchain.chains.conversation.memory import ConversationBufferWindowMemory\n",
        "from langchain_groq import ChatGroq\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    This function is the main entry point of the application. It sets up the Groq client, the Streamlit interface, and handles the chat interaction.\n",
        "    \"\"\"\n",
        "\n",
        "    # Get Groq API key\n",
        "    groq_api_key = \"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\"\n",
        "    model = 'llama3-8b-8192'\n",
        "    # Initialize Groq Langchain chat object and conversation\n",
        "    groq_chat = ChatGroq(\n",
        "            groq_api_key=groq_api_key,\n",
        "            model_name=model\n",
        "    )\n",
        "\n",
        "    print(\"Hello! I'm your friendly Groq chatbot. I can help answer your questions, provide information, or just chat. I'm also super fast! Let's start our conversation!\")\n",
        "\n",
        "    system_prompt = 'You are a friendly conversational chatbot'\n",
        "    conversational_memory_length = 5 # number of previous messages the chatbot will remember during the conversation\n",
        "\n",
        "    memory = ConversationBufferWindowMemory(k=conversational_memory_length, memory_key=\"chat_history\", return_messages=True)\n",
        "\n",
        "\n",
        "    #chat_history = []\n",
        "    while True:\n",
        "        user_question = input(\"Ask a question: \")\n",
        "        user_question = enhanced_question(user_question)\n",
        "        #chat_history.append({\"role\": \"user\", \"content\": user_question})\n",
        "\n",
        "        # user_question = enhanced_question(user_question)\n",
        "\n",
        "        # If the user has asked a question,\n",
        "        if user_question:\n",
        "\n",
        "            # Construct a chat prompt template using various components\n",
        "            prompt = ChatPromptTemplate.from_messages(\n",
        "                [\n",
        "                    SystemMessage(\n",
        "                        content=system_prompt\n",
        "                    ),  # This is the persistent system prompt that is always included at the start of the chat.\n",
        "\n",
        "                    MessagesPlaceholder(\n",
        "                        variable_name=\"chat_history\"\n",
        "                    ),  # This placeholder will be replaced by the actual chat history during the conversation. It helps in maintaining context.\n",
        "\n",
        "                    HumanMessagePromptTemplate.from_template(\n",
        "                        \"{human_input}\"\n",
        "                    ),  # This template is where the user's current input will be injected into the prompt.\n",
        "                ]\n",
        "            )\n",
        "\n",
        "            # Create a conversation chain using the LangChain LLM (Language Learning Model)\n",
        "            conversation = LLMChain(\n",
        "                llm=groq_chat,  # The Groq LangChain chat object initialized earlier.\n",
        "                prompt=prompt,  # The constructed prompt template.\n",
        "                verbose=False,   # TRUE Enables verbose output, which can be useful for debugging.\n",
        "                memory=memory,  # The conversational memory object that stores and manages the conversation history.\n",
        "            )\n",
        "            # The chatbot's answer is generated by sending the full prompt to the Groq API.\n",
        "            response = conversation.predict(human_input=user_question)\n",
        "            print(\"Chatbot:\", response)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 723
        },
        "id": "tj5BKX2PSciV",
        "outputId": "80fa7e54-9d6f-4cab-f691-87826de3faf9"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello! I'm your friendly Groq chatbot. I can help answer your questions, provide information, or just chat. I'm also super fast! Let's start our conversation!\n",
            "Ask a question: What are some milestone model architectures and papers in the last few years?\n",
            "['https://stanford-cs324.github.io/winter2022/lectures/environment/', 'https://raw.githubusercontent.com/Hannibal046/Awesome-LLM/main/README.md']\n",
            "Chatbot: What a fascinating topic! You're interested in exploring the milestone model architectures and papers in the last few years, primarily focusing on the intersection of artificial intelligence (AI) and climate change. I'd be delighted to help you explore this area.\n",
            "\n",
            "To start with, let's focus on the climate change aspect. Did you know that the average global temperature has increased by 2.14˚F (1.19˚C) since 1900? The top 10 warmest years have all occurred since 2005. Rising temperatures are causing natural disasters like heatwaves, floods, and wildfires, and also threatening coastal communities and ecosystems.\n",
            "\n",
            "Now, let's dive into the AI and climate change intersection. There are several papers and resources that examine the impact of AI on the environment, including carbon footprint reduction and climate modeling. Some notable papers include:\n",
            "\n",
            "* \"Environmental Impact of Artificial Intelligence\" by the University of Cambridge (2020)\n",
            "* \"Assessing the Environmental Impact of AI\" by the University of Oxford (2020)\n",
            "* \"Climate Change and the Future of AI\" by the World Economic Forum (2020)\n",
            "\n",
            "As for milestone model architectures in the last few years, some notable ones include:\n",
            "\n",
            "* BERT (2018): A language model that achieved state-of-the-art results in a wide range of natural language processing (NLP) tasks.\n",
            "* RoBERTa (2019): A variant of BERT that achieved even better results, particularly in the area of language translation.\n",
            "* DistilBERT (2019): A smaller, more efficient version of BERT that is easier to fine-tune and deploy.\n",
            "* T5 (2020): A text-to-text transformer model that can generate text in various styles and formats.\n",
            "* PaLM (2022): A massive language model that achieved state-of-the-art results in a wide range of NLP tasks and has been used for various applications, including natural language understanding and text generation.\n",
            "\n",
            "These are just a few examples of the many milestone model architectures and papers in the last few years. I hope this helps you get started on your exploration of the intersection of AI and climate change!\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-3e23255a459f>\u001b[0m in \u001b[0;36m<cell line: 75>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-23-3e23255a459f>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;31m#chat_history = []\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0muser_question\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Ask a question: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0muser_question\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menhanced_question\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_question\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;31m#chat_history.append({\"role\": \"user\", \"content\": user_question})\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Method 2\n",
        "\n",
        "This method utilize the approach of providing all the data to the LLM at once at starting so it already contains the whole data and then just answer the input queries from them."
      ],
      "metadata": {
        "id": "XcqwPaNJAP4H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''Not tried because data is huge and GROQ api can't handle this much data,\n",
        "but this approach will surely solve your issue of providing data to LLMs and in\n",
        "return get the relevant links, url, paragraph from where data is recovered\n",
        "'''\n",
        "import os\n",
        "\n",
        "from langchain.chains import LLMChain\n",
        "from langchain_core.prompts import (\n",
        "    ChatPromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        "    MessagesPlaceholder,\n",
        ")\n",
        "from langchain_core.messages import SystemMessage\n",
        "from langchain.chains.conversation.memory import ConversationBufferWindowMemory\n",
        "from langchain_groq import ChatGroq\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    This function is the main entry point of the application. It sets up the Groq client, the Streamlit interface, and handles the chat interaction.\n",
        "    \"\"\"\n",
        "\n",
        "    # Get Groq API key\n",
        "    groq_api_key = \"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\"\n",
        "    model = 'llama3-8b-8192'\n",
        "    # Initialize Groq Langchain chat object and conversation\n",
        "    groq_chat = ChatGroq(\n",
        "            groq_api_key=groq_api_key,\n",
        "            model_name=model\n",
        "    )\n",
        "\n",
        "    print(\"Hello! I'm your friendly Groq chatbot. I can help answer your questions, provide information, or just chat. I'm also super fast! Let's start our conversation!\")\n",
        "\n",
        "    system_prompt = 'You are a friendly conversational chatbot'\n",
        "    conversational_memory_length = 5 # number of previous messages the chatbot will remember during the conversation\n",
        "\n",
        "    memory = ConversationBufferWindowMemory(k=conversational_memory_length, memory_key=\"chat_history\", return_messages=True)\n",
        "\n",
        "    user_question = '''I am going to provide you some data which contains url from where data is taken,\n",
        "    paragraph number, content of parargraph, relevant links and tables.\n",
        "    Kindly give the answers on the basis of these data along with url of lectures provided,\n",
        "    paragraph number of content, related links and content but and here is the whole data! \\n'''\n",
        "    prompt = ChatPromptTemplate.from_messages(\n",
        "              [\n",
        "                  SystemMessage(\n",
        "                      content=system_prompt\n",
        "                  ),  # This is the persistent system prompt that is always included at the start of the chat.\n",
        "                MessagesPlaceholder(\n",
        "                      variable_name=\"chat_history\"\n",
        "                  ),  # This placeholder will be replaced by the actual chat history during the conversation. It helps in maintaining context.\n",
        "                HumanMessagePromptTemplate.from_template(\n",
        "                      \"{human_input}\"\n",
        "                  ),  # This template is where the user's current input will be injected into the prompt.\n",
        "              ]\n",
        "          )\n",
        "    conversation = LLMChain(\n",
        "                llm=groq_chat,  # The Groq LangChain chat object initialized earlier.\n",
        "                prompt=prompt,  # The constructed prompt template.\n",
        "                verbose=False,   # TRUE Enables verbose output, which can be useful for debugging.\n",
        "                memory=memory,  # The conversational memory object that stores and manages the conversation history.\n",
        "            )\n",
        "    for item in sections:\n",
        "        current_text = \"\"\n",
        "        current_text += f\"Lecture Url: {item['lecture_url']} \\n\"\n",
        "        current_text += f\"Paragraph Number of content: {item['paragraph_number']} \\n\"\n",
        "        current_text += f\"Content: {item['content']} \\n\"\n",
        "        current_text += f\"Links related to content: {item['links']} \\n\"\n",
        "        current_text += f\"Tables related to content: {item['tables']} \\n\\n\"\n",
        "        prompt = ChatPromptTemplate.from_messages(\n",
        "              [\n",
        "                  SystemMessage(\n",
        "                      content=system_prompt\n",
        "                  ),  # This is the persistent system prompt that is always included at the start of the chat.\n",
        "                MessagesPlaceholder(\n",
        "                      variable_name=\"chat_history\"\n",
        "                  ),  # This placeholder will be replaced by the actual chat history during the conversation. It helps in maintaining context.\n",
        "                HumanMessagePromptTemplate.from_template(\n",
        "                      \"{human_input}\"\n",
        "                  ),  # This template is where the user's current input will be injected into the prompt.\n",
        "              ]\n",
        "        )\n",
        "        conversation = LLMChain(\n",
        "                llm=groq_chat,  # The Groq LangChain chat object initialized earlier.\n",
        "                prompt=prompt,  # The constructed prompt template.\n",
        "                verbose=False,   # TRUE Enables verbose output, which can be useful for debugging.\n",
        "                memory=memory,  # The conversational memory object that stores and manages the conversation history.\n",
        "          )\n",
        "\n",
        "\n",
        "    #chat_history = []\n",
        "    while True:\n",
        "        user_question = input(\"Ask a question: \")\n",
        "        user_question = enhanced_question(user_question)\n",
        "        #chat_history.append({\"role\": \"user\", \"content\": user_question})\n",
        "\n",
        "        user_question = enhanced_question(user_question)\n",
        "\n",
        "        # If the user has asked a question,\n",
        "        if user_question:\n",
        "\n",
        "            # Construct a chat prompt template using various components\n",
        "            prompt = ChatPromptTemplate.from_messages(\n",
        "                [\n",
        "                    SystemMessage(\n",
        "                        content=system_prompt\n",
        "                    ),  # This is the persistent system prompt that is always included at the start of the chat.\n",
        "\n",
        "                    MessagesPlaceholder(\n",
        "                        variable_name=\"chat_history\"\n",
        "                    ),  # This placeholder will be replaced by the actual chat history during the conversation. It helps in maintaining context.\n",
        "\n",
        "                    HumanMessagePromptTemplate.from_template(\n",
        "                        \"{human_input}\"\n",
        "                    ),  # This template is where the user's current input will be injected into the prompt.\n",
        "                ]\n",
        "            )\n",
        "\n",
        "            # Create a conversation chain using the LangChain LLM (Language Learning Model)\n",
        "            conversation = LLMChain(\n",
        "                llm=groq_chat,  # The Groq LangChain chat object initialized earlier.\n",
        "                prompt=prompt,  # The constructed prompt template.\n",
        "                verbose=False,   # TRUE Enables verbose output, which can be useful for debugging.\n",
        "                memory=memory,  # The conversational memory object that stores and manages the conversation history.\n",
        "            )\n",
        "            # The chatbot's answer is generated by sending the full prompt to the Groq API.\n",
        "            response = conversation.predict(human_input=user_question)\n",
        "            print(\"Chatbot:\", response)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "-Kuz8g5NARWl",
        "outputId": "550ba775-1188-4489-9d0e-58a8257645f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello! I'm your friendly Groq chatbot. I can help answer your questions, provide information, or just chat. I'm also super fast! Let's start our conversation!\n",
            "Ask a question: What about the Content moderation in LLMs?\n",
            "Chatbot: You're asking about content moderation in Large Language Models (LLMs). Before diving into LLMs, it's essential to understand the critical issue of content moderation.\n",
            "\n",
            "Content moderation is the process of regulating and managing user-generated content on online platforms. This includes social media sites like Facebook, Twitter, and YouTube, which are constantly battling harmful content, such as hate speech, harassment, pornography, violence, fraud, disinformation, and copyright infringement.\n",
            "\n",
            "To tackle this issue, companies have employed AI to automate content moderation. The outcome of moderation can be either hard (blocking or deletion) or soft (flagging or hiding). However, the decision on what is allowed is fundamentally political, as it depends on various factors, such as what constitutes harmful content, which is context-dependent.\n",
            "\n",
            "The study by Chandrasekhran et al. (2018) on Reddit highlights the complexity of content moderation. They analyzed 2.8M removed comments from 100 subreddits over 10 months and found that norms vary across different subreddits. While there are common norms, many are specific to subreddits.\n",
            "\n",
            "Now, let's talk about the dual use of language models in the context of toxicity and disinformation. Language models can be used to either:\n",
            "\n",
            "1. **Generate** toxic content, which malicious actors can use to amplify their message.\n",
            "2. **Detect** disinformation and aid in content moderation, helping to keep online spaces safe.\n",
            "\n",
            "The links provided offer more insights into the challenges of content moderation:\n",
            "\n",
            "1. Facebook's Community Standards: https://transparency.fb.com/policies/community-standards/\n",
            "2. Interviews with Facebook content moderators: https://www.theverge.com/2019/2/25/18229714/cognizant-facebook-content-moderator-interviews-trauma-working-conditions-arizona\n",
            "3. A research paper on the study by Chandrasekhran et al. (2018): https://dl.acm.org/doi/pdf/10.1145/3274301\n",
            "\n",
            "In summary, content moderation is a critical issue in the context of LLMs. AI has been employed to automate moderation, but the decision on what is allowed remains political and context-dependent.\n",
            "Ask a question: What school did burne hogarth establish?\n",
            "Chatbot: According to the provided information, Burne Hogarth established the School of Visual Arts.\n",
            "Ask a question: Who played tess on touched by an angel?\n",
            "Chatbot: According to the provided information, Della Reese played Tess on \"Touched by an Angel\".\n",
            "Ask a question: The dates?\n",
            "Chatbot: The dates mentioned in the text are:\n",
            "\n",
            "* 1508: The year when the BPE algorithm was first developed.\n",
            "* 2015: The year when Sennrich et al. applied the BPE algorithm to produce one of the most commonly used tokenizers.\n",
            "* 2019: The year when Wang et al. applied the BPE algorithm to bytes instead of Unicode characters.\n",
            "\n",
            "Additionally, the text mentions the birth and death dates of Della Reese, who played Tess on \"Touched by an Angel\":\n",
            "\n",
            "* July 6, 1931: Della Reese's birthdate.\n",
            "* November 19, 2017: Della Reese's date of death.\n",
            "Ask a question: Mein Haus liegt auf dem Hügel. = ?\n",
            "Chatbot: The translation is:\n",
            "\n",
            "Mein Haus liegt auf dem Hügel. = My house is on the hill.\n",
            "Ask a question: Give me complete article of this United Methodists Agree to Historic Split\n",
            "Chatbot: Here is the complete article:\n",
            "\n",
            "**United Methodists Agree to Historic Split**\n",
            "\n",
            "**Those who oppose gay marriage will form their own denomination**\n",
            "\n",
            "After two days of intense debate, the United Methodist Church has agreed to a historic split - one that is expected to end in the creation of a new denomination, one that will be \"theologically and socially conservative,\" according to The Washington Post.\n",
            "\n",
            "The majority of delegates attending the church's annual General Conference in May voted to strengthen a ban on the ordination of LGBTQ clergy and to write new rules that will \"discipline\" clergy who officiate at same-sex weddings. But those who opposed these measures have a new plan: They say they will form a separate denomination by 2020, calling their church the Christian Methodist denomination.\n",
            "\n",
            "The move is expected to create a significant rift within the church, with some members expressing disappointment and frustration at the decision to split. Others, however, have welcomed the move as a necessary step to preserve the church's traditional teachings.\n",
            "\n",
            "The decision comes after years of debate and controversy over the church's stance on LGBTQ issues, with some members pushing for greater inclusivity and acceptance. However, the majority of delegates at the General Conference opposed these efforts, instead opting to maintain the church's traditional stance on the issue.\n",
            "\n",
            "The split is expected to be completed by 2020, with the new denomination likely to be formed in the coming months.\n",
            "Ask a question: Poor English inputS in data?\n",
            "Chatbot: The input sentences that are considered \"Poor English\" include:\n",
            "\n",
            "1. Poor English input: I eated the purple berries.\n",
            "Good English output: I ate the purple berries.\n",
            "\n",
            "2. Poor English input: Thank you for picking me as your designer. I’d appreciate it.\n",
            "Good English output: Thank you for choosing me as your designer. I appreciate it.\n",
            "\n",
            "3. Poor English input: The mentioned changes have done. or I did the alteration that yourequested. or I changed things you wanted and did the modifications.\n",
            "Good English output: The requested changes have been made. or I made the alteration that yourequested. or I changed things you wanted and made the modifications.\n",
            "\n",
            "4. Poor English input: I’d be more than happy to work with you in another project.\n",
            "Good English output: I would be happy to work with you on another project.\n",
            "\n",
            "These sentences contain grammatical errors, such as incorrect verb tenses, incorrect word choice, and incorrect sentence structure. The \"Good English\" outputs correct these errors and provide grammatically correct sentences.\n",
            "Ask a question: Lecture url of this poor and good inputs outputs?\n",
            "Chatbot: The lecture URL is:\n",
            "\n",
            "http://crfm-models.stanford.edu/static/index.html?prompt=Poor%20English%20input%3A%20I%20eated%20the%20purple%20berries.%0AGood%20English%20output%3A%20I%20ate%20the%20purple%20berries.%0APoor%20English%20input%3A%20Thank%20you%20for%20picking%20me%20as%20your%20designer.%20I%E2%80%99d%20appreciate%20it.%0AGood%20English%20output%3A%20Thank%20you%20for%20choosing%20me%20as%20your%20designer.%20I%20appreciate%20it.%0APoor%20English%20input%3A%20The%20mentioned%20changes%20have%20done.%20or%20I%20did%20the%20alteration%20that%20you%0Arequested.%20or%20I%20changed%20things%20you%20wanted%20and%20did%20the%20modifications.%0AGood%20English%20output%3A%20The%20requested%20changes%20have%20been%20made.%20or%20I%20made%20the%20alteration%20that%20you%0Arequested.%20or%20I%20changed%20things%20you%20wanted%20and%20made%20the%20modifications.%0APoor%20English%20input%3A%20I%E2%80%99d%20be%20more%20than%20happy%20to%20work%20with%20you%20in%20another%20project.%0AGood%20English%20output%3A&settings=stop_sequences%3A%20%5B%5Cn%5D%0Atemperature%3A%200%0Atop_k_per_token%3A%205&environments=\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-51-9a3b83318dcf>\u001b[0m in \u001b[0;36m<cell line: 129>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-51-9a3b83318dcf>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;31m#chat_history = []\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m         \u001b[0muser_question\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Ask a question: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0muser_question\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menhanced_question\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_question\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;31m#chat_history.append({\"role\": \"user\", \"content\": user_question})\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    }
  ]
}